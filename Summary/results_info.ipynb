{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.gridspec as gridspec\n",
    "from math import pi\n",
    "\n",
    "\n",
    "\n",
    "rcParams['figure.dpi'] = 500\n",
    "rcParams['savefig.dpi'] = 500\n",
    "rcParams['font.family'] = 'Arial'\n",
    "rcParams['axes.labelsize'] = 18\n",
    "rcParams['axes.titlesize'] = 18\n",
    "rcParams['legend.fontsize'] = 18\n",
    "rcParams['figure.titlesize'] = 18\n",
    "rcParams['markers.fillstyle'] = 'none'\n",
    "\n",
    "\n",
    "# Map question code to difficulty level\n",
    "# Used in the yield_data function\n",
    "# The BSP is SAS, such rename happens in after data is loaded and summrized\n",
    "problem_nphard_mapper = {\n",
    "    'SPP': 'p', 'MFP': 'p', 'BSP': 'p', 'EDP': 'p',\n",
    "    'TSP_D': 'np-cmp', 'GCP_D': 'np-cmp', 'KSP': 'np-cmp',\n",
    "    'TSP': 'np-hard', 'GCP': 'np-hard', 'MSP': 'np-hard',\n",
    "}\n",
    "\n",
    "nphard_order_mapper = {'np-hard': 2, 'np-cmp': 1, 'p': 0}\n",
    "\n",
    "\n",
    "# Order of questions in graphs\n",
    "problem_order_mapper = {\n",
    "    'GCP': 0, 'TSP': 1, 'MSP': 2, 'GCP_D': 3, 'TSP_D': 4, 'KSP': 5, 'SAS': 6, 'EDP': 7, 'SPP': 8\n",
    "}\n",
    "\n",
    "# Order of models in graphs\n",
    "model_order_mapper = {\"Gemini\": 0, \"GPT4V\": 1, \"LLaVa\":2, \"Otter\":3 ,\"Qwen-VL\": 4, \n",
    "                       \"CogVLM\":5 ,\"BLIP-2\": 6, \"Fuyu-8b\": 7, \"Kosmos2\": 8,\n",
    "                       \"Close models\":9, \"Open models\" : 10, \"All models\":11}\n",
    "\n",
    "model_types = [\"Open\", \"Close\"]\n",
    "prompt_types = [\"figure+full_text\", \"figure+limited_text\", \"full_text_only\", \"recognition\"]\n",
    "\n",
    "\n",
    "\n",
    "def yield_data(data):\n",
    "    \"\"\"Generate the data for the detailed dataframe,\n",
    "       data is dict of dict containing parsed experiment data\n",
    "    \"\"\"\n",
    "    model_name = data[\"model_name\"]\n",
    "    problem_name = data[\"problem_name\"]\n",
    "    model_type = data[\"model_type\"]\n",
    "    prompt_type = data[\"prompt_type\"]\n",
    "\n",
    "    \n",
    "    try:\n",
    "        problem_nphard_mapper[problem_name]\n",
    "    except:\n",
    "        print('in yield_data except')\n",
    "        print(problem_name)\n",
    "        print(data)\n",
    "        print()\n",
    "        \n",
    "    return [\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"problem_name\": problem_name,\n",
    "            \"problem_type\": problem_nphard_mapper[problem_name],\n",
    "            # 1 is close model and 0 is open model\n",
    "            \"model_type\": int(model_type == \"Close\"),\n",
    "            \"prompt_type\": prompt_type,\n",
    "            \"recognized\": data[\"recognized\"][i],\n",
    "            \"parsable\": data[\"parsable\"][i],\n",
    "            \"correctness\": data[\"correctness\"][i],\n",
    "            \"recognized_and_parsable\": data[\"recognized_and_parsable\"][i],\n",
    "            \"level\": data[\"level\"][i]\n",
    "        } for i in range(10) \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsInfo:\n",
    "    \"\"\"Wrapper class for the dataset\"\"\"\n",
    "    def __init__(self, DATA_PATH, model_types, prompt_types, level_range, save_path=None):\n",
    "        \"\"\"initizalize the dataset\"\"\"\n",
    "        assert os.path.exists(DATA_PATH), f\"{DATA_PATH} does not exist\"\n",
    "        self.main_dir = DATA_PATH\n",
    "        self.model_type_dirs = []\n",
    "        self.prompt_type_dirs = []\n",
    "        self.data = []\n",
    "        self.level_range = level_range\n",
    "        self.fetch_model_types(model_types)\n",
    "        self.fetch_prompt_types(prompt_types)\n",
    "        self.fetch_exist_results()\n",
    "        self.dataframe = None\n",
    "        self.convert_to_dataframe()\n",
    "        self.rename_model_prompt_type()\n",
    "        # save the dataframe\n",
    "        if save_path:\n",
    "            self.dataframe.to_csv(save_path, index=False)\n",
    "\n",
    "    def fetch_model_types(self, model_types):\n",
    "        \"\"\"Fetch the existing model type directories(Open or Close)\"\"\"\n",
    "        for model_type in model_types:\n",
    "            datapath = os.path.join(self.main_dir, model_type)\n",
    "            if os.path.exists(datapath):\n",
    "                self.model_type_dirs.append(model_type)\n",
    "    \n",
    "    def fetch_prompt_types(self, prompt_types):\n",
    "        \"\"\"For each model type, fetch the existing prompt type directories\"\"\"\n",
    "        for model_type_dir in self.model_type_dirs:\n",
    "            for prompt_type in prompt_types:\n",
    "                datapath = os.path.join(self.main_dir, model_type_dir, prompt_type)\n",
    "                if os.path.exists(datapath):\n",
    "                    self.prompt_type_dirs.append((model_type_dir, prompt_type))\n",
    "    \n",
    "    def fetch_exist_results(self):\n",
    "        \"\"\"Iterate through the directories and fetch the json results and metadata\"\"\"\n",
    "        for model_type, prompt_type in tqdm(self.prompt_type_dirs):\n",
    "            datapath = os.path.join(self.main_dir, model_type, prompt_type)\n",
    "            for file in os.listdir(datapath):\n",
    "                if file.endswith(\".json\"):\n",
    "                    self.fetch_json_result(file, model_type, prompt_type, datapath)\n",
    "    \n",
    "    def fetch_json_result(self, file, model_type, prompt_type, datapath):\n",
    "        \"\"\"Fetch the result and extract the metadata from one json file\"\"\"\n",
    "        metadata = self.parse_json_filename(file)\n",
    "        metadata.update({\"model_type\": model_type, \"prompt_type\": prompt_type})\n",
    "        data = None\n",
    "        try:\n",
    "            with open(os.path.join(datapath, file), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            if prompt_type == \"recognition\":\n",
    "                data_info = self.recognition_data_info_extractor(data, metadata)\n",
    "            else:\n",
    "                data_info = self.data_info_extractor(data, metadata)\n",
    "            data_info.update(metadata)\n",
    "            self.data.append(data_info)\n",
    "        except Exception as e:\n",
    "            # continue to run and find all the errored files \n",
    "            print(f\"Error in fetching {file}: {e}\")\n",
    "            print(f\"metadata: {metadata}\")\n",
    "            print()\n",
    "            pass\n",
    "\n",
    "    def parse_json_filename(self, filename):\n",
    "        \"\"\"Parse the filename to extract the model and problem names to get the metadata\n",
    "           Assume the following naming <question name>_<model name>_results.json\n",
    "           the question name could be <question name> or <question name>_D\n",
    "        \"\"\"\n",
    "        filename = filename.split(\".\")[0]\n",
    "        filename = filename.split(\"_\")\n",
    "        raw_model_name = filename[-2]\n",
    "        raw_problem_name = \"_\".join(filename[:-2])\n",
    "\n",
    "\n",
    "        # rename blip and gemini model \n",
    "        if raw_model_name == \"Blip\" or raw_model_name == \"blip\" or raw_model_name == \"blip2\":\n",
    "            raw_model_name = \"BLIP-2\"\n",
    "        elif raw_model_name == \"gemini\" :\n",
    "            raw_model_name = \"Gemini\"\n",
    "        return {\"model_name\": raw_model_name, \"problem_name\": raw_problem_name}\n",
    "\n",
    "    def data_postprocessor(self, data, metadata):\n",
    "        \"\"\"\n",
    "        Postprocess the data to extract the required information because the data is \n",
    "        not in the same format\n",
    "        \"\"\"\n",
    "        standard_data = []\n",
    "        for x in data:\n",
    "\n",
    "            # since the Gemini output could include an extra dict\n",
    "            # \"id\" and \"object\" is used to skip such dict \n",
    "            # a vaild result should only contain \"output\" \"correctness\" \"reasoning\" as key\n",
    "            if \"id\" and \"object\" in x:\n",
    "                continue\n",
    "\n",
    "            # the dtype of \"correctness\" could be bool or [bool, \"comment on output\"]\n",
    "            x[\"recognized\"] = x[\"correctness\"] is not None\n",
    "            x[\"correctness\"] = x[\"correctness\"][0] if isinstance(x[\"correctness\"], list) else x[\"correctness\"]\n",
    "            # when there is output, such result is regraded as parsable\n",
    "            x[\"parsable\"] = (x[\"output\"] != \"\")\n",
    "\n",
    "            standard_data.append(x)\n",
    "        return standard_data\n",
    "\n",
    "    def recognition_data_info_extractor(self, data, metadata):\n",
    "        # the recognition of a question is the mean five recognition tests\n",
    "        # in each test, 1 is success and 0 is fail\n",
    "        # the level recognition is the mean of the recognition of the question in the level \n",
    "        recognized_summary = np.array([x[\"mean\"] for x in data]).reshape(-1, 10).sum(axis=-1)\n",
    "        return {\n",
    "            \"recognized\": recognized_summary,\n",
    "            \"parsable\": recognized_summary,\n",
    "            \"correctness\": recognized_summary,\n",
    "            \"recognized_and_parsable\": recognized_summary,\n",
    "            \"level\": self.level_range\n",
    "        }\n",
    "\n",
    "    def data_info_extractor(self, data, metadata):\n",
    "        \"\"\"\n",
    "        For each data and metadata, extract the required information from the data\n",
    "        \"\"\"\n",
    "        standard_data = self.data_postprocessor(data, metadata)\n",
    "        data_summary = self.data_summary(standard_data)\n",
    "        return data_summary\n",
    "    \n",
    "    def data_summary(self, data):\n",
    "        \"\"\"Summarize the data\"\"\"\n",
    "        recognized_data = np.array([x[\"recognized\"] for x in data]).reshape(-1, 10)\n",
    "        parsable_data = np.array([x[\"parsable\"] for x in data]).reshape(-1, 10)\n",
    "        correctness_data = np.array([x[\"correctness\"] for x in data]).reshape(-1, 10)\n",
    "\n",
    "        # sum of stats of a question at a difficulty level\n",
    "        recognaized_summary = np.sum(recognized_data, axis=-1)\n",
    "        failure_summary = np.sum(parsable_data, axis=-1)\n",
    "        number_of_recognized_and_parsable = np.sum(recognized_data * parsable_data, axis=-1)\n",
    "        correctness_summary = np.sum(correctness_data, axis=-1)\n",
    "        return {\n",
    "            \"recognized\": recognaized_summary,\n",
    "            \"parsable\": failure_summary,\n",
    "            \"correctness\": correctness_summary,\n",
    "            \"recognized_and_parsable\": number_of_recognized_and_parsable,\n",
    "            \"level\": self.level_range\n",
    "        }\n",
    "\n",
    "    def convert_to_dataframe(self):\n",
    "        \"\"\"Convert the data to a pandas dataframe\"\"\"\n",
    "\n",
    "        ###########################################################\n",
    "        # more informative error \n",
    "        try:\n",
    "            all_data = [a for x in self.data for a in yield_data(x)]\n",
    "            self.dataframe = pd.DataFrame(all_data)\n",
    "        except Exception as e:\n",
    "            print(\"error in convert_to_dataframe\")\n",
    "            print(f\"e is  {e}\")\n",
    "            print(\"all data:\")\n",
    "            print(self.data)\n",
    "            raise e\n",
    "\n",
    "\n",
    "    # rename the raw model name in dataframe\n",
    "    def rename_model_prompt_type(self):\n",
    "        model_replace_dict = {\n",
    "            \"fuyu\": \"Fuyu-8b\",\n",
    "            \"qwen\": \"Qwen-VL\",\n",
    "            \"gemini\": \"Gemini\",\n",
    "            \"Blip2\": \"BLIP-2\",\n",
    "            \"blip2\": \"BLIP-2\",  \n",
    "            \"Llava\": \"LLaVa\",\n",
    "            \"llava\": \"LLaVa\",  \n",
    "            \"Cogvlm\": \"CogVLM\",\n",
    "            \"cogvlm\": \"CogVLM\",\n",
    "            \"Gpt4V\": \"GPT4V\",\n",
    "            \"Gpt4v\": \"GPT4V\",\n",
    "            \"gpt4v\": \"GPT4V\",\n",
    "            \"kosmos2\":\"Kosmos2\",\n",
    "            \"otter\": \"Otter\"\n",
    "\n",
    "        }\n",
    "\n",
    "        self.dataframe[\"model_name\"] = self.dataframe[\"model_name\"].replace(model_replace_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizeHelper:\n",
    "    \"\"\"Helper class to summarize the data to prepare for the visualization\"\"\"\n",
    "    def __init__(self, dataframe, summary_func):\n",
    "        self.dataframe = dataframe\n",
    "        self.summary_func = summary_func\n",
    "    \n",
    "    # add summary_columns = None\n",
    "    def summarize(self, summary_columns= None, groupby_columns=None, pre_func=None, post_func=None):\n",
    "        # TO-DO: to be refined\n",
    "        dataframe = self.dataframe.copy()\n",
    "        if pre_func:\n",
    "            dataframe = pre_func(dataframe)\n",
    "        if groupby_columns:\n",
    "            column_used = groupby_columns + summary_columns\n",
    "            dataframe = dataframe[column_used].groupby(groupby_columns, as_index=False).agg({\n",
    "                col: self.summary_func[col] for col in summary_columns\n",
    "            }).reset_index()\n",
    "        if post_func:\n",
    "            dataframe = post_func(dataframe)\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "def pre_func_for_agg_acc(dataframe):\n",
    "    dataframe_prompt = dataframe[dataframe[\"prompt_type\"] != \"recognition\"].copy()\n",
    "    dataframe_recognition = dataframe[dataframe[\"prompt_type\"] == \"recognition\"].copy()\n",
    "    # Obtain the RA, 10 is the number of question at a difficulty level\n",
    "    dataframe_recognition[\"RA\"] = dataframe_recognition[\"correctness\"] / 10\n",
    "    # Obtain the Acc, correctness in recognized and parsable question\n",
    "    dataframe_prompt[\"AA\"] = dataframe_prompt[\"correctness\"] / dataframe_prompt[\"recognized_and_parsable\"].clip(0.1, 10)\n",
    "    dataframe_prompt[\"FR\"] = 1 - dataframe_prompt[\"parsable\"] / 10\n",
    "    dataframe_prompt[\"Acc\"] = dataframe_prompt[\"AA\"] * (1 - dataframe_prompt[\"FR\"])\n",
    "    # Obtain the weighted_acc\n",
    "    # the RA data is derived from the recognition data set\n",
    "    dataframe_prompt[\"RA\"] = 0.0\n",
    "\n",
    "    # match recognition data to its experiment data\n",
    "    # could be refined using join \n",
    "    for i, row in dataframe_prompt.iterrows():\n",
    "        related_value = dataframe_recognition[\n",
    "            (dataframe_recognition[\"model_name\"] == row[\"model_name\"]) & \n",
    "            (dataframe_recognition[\"problem_name\"] == row[\"problem_name\"]) &\n",
    "            (dataframe_recognition[\"level\"] == row[\"level\"])\n",
    "        ][\"RA\"].values\n",
    "\n",
    "        if len(related_value) != 1:\n",
    "            print(\"error matching\")\n",
    "            print(f\"index:{i}\")\n",
    "            print(f\"row:{row}\")\n",
    "            print(\"data frane\")\n",
    "            print(dataframe)\n",
    "            print()\n",
    "\n",
    "        dataframe_prompt.loc[i, \"RA\"] = related_value[0] if len(related_value) > 0 else 1\n",
    "    dataframe_prompt[\"weighted_acc\"] = dataframe_prompt[\"Acc\"] * dataframe_prompt[\"RA\"]\n",
    "    return dataframe_prompt\n",
    "\n",
    "\n",
    "\n",
    "# apply weight of level \n",
    "def pre_func_common(data):\n",
    "    dataframe = data.copy()\n",
    "    dataframe[\"weighted_acc\"] = dataframe[\"weighted_acc\"] * dataframe[\"level\"]\n",
    "    return dataframe\n",
    "\n",
    "# the weight of a level is apply using the mean form\n",
    "def post_func_common(data):\n",
    "    data[\"weighted_acc\"] = data[\"weighted_acc\"] / data[\"level\"]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrawHelper:\n",
    "    \"\"\"Helper class to draw the results from the dataframe\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def plot_bar_chart(self, x_name, y_name, hue_name):\n",
    "        sns.barplot(data=self.dataframe, x=x_name, y=y_name, hue=hue_name)\n",
    "        plt.title(\"\")\n",
    "        plt.ylabel(\"Aggregated Accuracy\")\n",
    "        plt.legend(title='Prompt Type',title_fontsize = 'x-large', fontsize='medium',markerscale=1)\n",
    "        plt.xlabel(\"\")\n",
    "\n",
    "    def plot_heatmap_recognition(self):\n",
    "        col = [\"model_name\",'problem_name',\"RA\"]\n",
    "        self.dataframe = self.dataframe[col]\n",
    "        pivot_table = self.dataframe.pivot(index='model_name', columns='problem_name', values='RA')\n",
    "        question_order = ['GCP', 'TSP', 'MSP', 'GCP_D', 'TSP_D', 'KSP', 'SAS', 'EDP', 'SPP']\n",
    "        pivot_table = pivot_table[question_order]\n",
    "        pivot_table = pivot_table.sort_index(key=lambda x: x.map(model_order_mapper))\n",
    "        sns.heatmap(pivot_table, annot=True, vmin=0, vmax=1, cmap='Blues', fmt='.2f')\n",
    "        plt.xlabel(None)\n",
    "        plt.ylabel(None)\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    def plot_heatmap(self, x_name, y_name, z_name, col_name, problem_order_mapper):\n",
    "        for z in self.dataframe[z_name].unique():\n",
    "            tmp_df = self.dataframe[self.dataframe[z_name] == z].copy()\n",
    "\n",
    "            # reorder level and model name  \n",
    "            tmp_df = tmp_df.pivot_table(index=x_name, columns=y_name, values=col_name, aggfunc='mean').fillna(0).reset_index()\n",
    "\n",
    "            # apply transpose() to set x be the level y be the model name\n",
    "            col = list(tmp_df[\"level\"])\n",
    "            tmp_df = tmp_df.transpose()\n",
    "            tmp_df = tmp_df.drop('level')\n",
    "            tmp_df.columns = col\n",
    "            tmp_df = tmp_df.sort_index(key=lambda x: x.map(model_order_mapper))\n",
    "\n",
    "            pos = problem_order_mapper[z] + 1\n",
    "            plt.subplot(3, 3, pos)\n",
    "            sns.heatmap(tmp_df, annot=True, vmin=0, vmax=1, cmap='Blues', fmt='.2f', cbar=False)\n",
    "            plt.title(z.upper())\n",
    "            plt.xlabel(None)\n",
    "            plt.ylabel(None)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "    def plot_spider(self, *metrics):\n",
    "        dataframe = self.dataframe.copy()\n",
    "        my_dpi = 100\n",
    "        fig = plt.figure(figsize=(3000/my_dpi, 3000/my_dpi), dpi=my_dpi)\n",
    "        \n",
    "        cols = 3\n",
    "        rows = 3\n",
    "        \n",
    "        gs = gridspec.GridSpec(rows, cols, wspace=0.5, hspace=0.5)\n",
    "        \n",
    "        for i, (index, row_data) in enumerate(dataframe.iterrows()):\n",
    "            if i >= rows * cols:  \n",
    "                break\n",
    "            ax = plt.subplot(gs[i], polar=True)  # Specify polar=True here\n",
    "            self.plot_sub_spider(row_data, ax, *metrics)\n",
    "\n",
    "            \n",
    "\n",
    "    def plot_sub_spider(self, row_data, ax, *metrics):\n",
    "\n",
    "        # here we assue five metrics \n",
    "        angles = [n / float(5) * 2 * pi for n in range(5)]\n",
    "        offset = pi / 5  \n",
    "        angles = [(angle + offset) % (2 * pi) for angle in angles]  \n",
    "\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        ax.set_theta_offset(pi / 2)\n",
    "        ax.set_theta_direction(-1)\n",
    "        \n",
    "        metrics = list(metrics)\n",
    "        metrics_tag = metrics.copy()\n",
    "        for i in range(len(metrics_tag)):\n",
    "            if metrics_tag[i] == 'AA NP Hard':\n",
    "                metrics_tag[i] = 'AA \\nNP \\nHard'\n",
    "            if metrics_tag[i] == 'AA P':\n",
    "                metrics_tag[i] = 'AA \\nP'\n",
    "            if metrics_tag[i] == 'AA NP Complete':\n",
    "                metrics_tag[i] = 'AA \\nNP Complete'\n",
    "        \n",
    "        plt.xticks(angles[:-1], metrics_tag, color='black', size=25)\n",
    "        ax.tick_params(axis='x', pad=30)\n",
    "        \n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.yticks([0,0.25,0.5,0.75,1], ['0','0.25','0.5','0.75','1'], color=\"grey\", size=18)\n",
    "        plt.ylim(0,1)\n",
    "        \n",
    "        model_name = row_data['model_name']\n",
    "        row_values = [row_data[metric] for metric in metrics]\n",
    "        row_values += row_values[:1]\n",
    "        \n",
    "        ax.plot(angles, row_values, linewidth=2, linestyle='solid')\n",
    "        ax.fill(angles, row_values, alpha=0.2)\n",
    "        \n",
    "        ax.set_title(model_name, size=45, y=1.1)  # Correctly set the title for the subplot\n",
    "        \n",
    "\n",
    "# this is not used now\n",
    "def gather_final_summary(base_info, cl_info, p_acc_info, npc_acc_info, nph_acc_info):\n",
    "    base_info = base_info.copy()\n",
    "    for i, row in base_info.iterrows():\n",
    "        model_name = row[\"model_name\"]\n",
    "        cl = cl_info[cl_info[\"model_name\"] == model_name][\"cl\"].values[0]\n",
    "        p_acc = p_acc_info[p_acc_info[\"model_name\"] == model_name][\"weighted_acc\"].values[0]\n",
    "        npc_acc = npc_acc_info[npc_acc_info[\"model_name\"] == model_name][\"weighted_acc\"].values[0]\n",
    "        nph_acc = nph_acc_info[nph_acc_info[\"model_name\"] == model_name][\"weighted_acc\"].values[0]\n",
    "        base_info.loc[i, \"cl\"] = cl\n",
    "        base_info.loc[i, \"p_acc\"] = p_acc\n",
    "        base_info.loc[i, \"npc_acc\"] = npc_acc\n",
    "        base_info.loc[i, \"nph_acc\"] = nph_acc\n",
    "    return base_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_chart(self, col_name, x_name, x_order_mapper,ax):\n",
    "        tmp_df = self.dataframe.copy()\n",
    "        tmp_df['x_order'] = tmp_df[x_name].map(x_order_mapper)\n",
    "        tmp_df.sort_values(by=['x_order', 'model_type'], inplace=True)\n",
    "\n",
    "        open_model_df = tmp_df[tmp_df['model_type'] == 0]\n",
    "        close_model_df = tmp_df[tmp_df['model_type'] == 1]\n",
    "\n",
    "        number_of_close_models = close_model_df['model_name'].nunique()\n",
    "        number_of_open_models = open_model_df['model_name'].nunique()\n",
    "\n",
    "        # make one red palette and one blue palette\n",
    "        palette = sns.color_palette('tab20', n_colors=number_of_close_models + number_of_open_models)\n",
    "        palette = sorted(palette, key=lambda x: x[0] - x[2])\n",
    "        palette_map = {}\n",
    "\n",
    "        sns.pointplot(data=open_model_df, ax = ax, x=x_name,\n",
    "                y=col_name, hue='model_name', linestyle='',\n",
    "                alpha=0.8, marker='s', palette=palette[:number_of_open_models])\n",
    "        palette_map = {model: palette[i] for i, model in enumerate(open_model_df['model_name'].unique())}\n",
    "\n",
    "        sns.pointplot(data=close_model_df, ax = ax, x=x_name, y=col_name,\n",
    "                        hue='model_name', linestyle='', alpha=0.8, marker='^', palette=palette[number_of_open_models:])\n",
    "        palette_map.update({model: palette[i + number_of_open_models] for i, model in enumerate(close_model_df['model_name'].unique())})\n",
    "\n",
    "        sns.lineplot(data=close_model_df, ax = ax, x=x_name, y=col_name, color='darkred',\n",
    "                        marker='o', markersize=10, fillstyle='full', label='Close models', errorbar=None)\n",
    "        \n",
    "        sns.lineplot(data=open_model_df, ax = ax, x=x_name, y=col_name, color='red',\n",
    "                        marker='o', markersize=10, fillstyle='full', label='Open models', errorbar=None)\n",
    "\n",
    "        sns.lineplot(data=tmp_df, ax = ax, x=x_name, y=col_name, color='black',\n",
    "                        marker='o', markersize=10, fillstyle='full', label='All models', errorbar=None)\n",
    "        \n",
    "        leg = ax.legend()\n",
    "\n",
    "        if col_name == 'weighted_acc':\n",
    "            plt.title(\"a.\", loc='left')\n",
    "            plt.ylabel('Aggregate Accuracy')\n",
    "            leg.remove()\n",
    "\n",
    "            \n",
    "        else:\n",
    "            plt.title(\"b.\", loc='left')\n",
    "            plt.ylabel('Instruction Following \\n Effective Rate')\n",
    "            # get the label and name and sort them based on model_order_mapper\n",
    "            handles, labels = plt.gca().get_legend_handles_labels()\n",
    "            sorted_labels_handles = sorted(zip(labels, handles), key=lambda lh: model_order_mapper.get(lh[0], 0))\n",
    "            sorted_labels, sorted_handles = zip(*sorted_labels_handles)\n",
    "        #     plt.legend(sorted_handles, sorted_labels,title='Model', bbox_to_anchor=(1.05, 1),\\\n",
    "        #                 markerscale = 1, fontsize = 'medium',loc='right')\n",
    "            ax.legend(sorted_handles, sorted_labels,title='Model',bbox_to_anchor=(1.05, 1),\\\n",
    "                         markerscale=1.3, fontsize='large',loc='upper left',title_fontsize = 'x-large')\n",
    "        # ax.legend(title='Label', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "        ax.set_ylim([0, 1])\n",
    "        plt.xlabel('Complexity')\n",
    "        # plt.tight_layout()\n",
    "        ax.set_xticklabels(['P','NP-Complete','NP-Hard'])\n",
    "        plt.ylim(0, 1.05)\n",
    "\n",
    "\n",
    "DrawHelper.plot_line_chart = plot_line_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 44.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Assume a new env is created \n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = os.path.join(base_dir, 'Results')\n",
    "\n",
    "# If not please use the following, and place the results in summary\n",
    "# DATA_PATH = 'Results'\n",
    "\n",
    "\n",
    "model_types = [\"Open\", \"Close\"]\n",
    "prompt_types = [\"figure+full_text\", \"figure+limited_text\", \"full_text_only\", \"recognition\"]\n",
    "level_range = np.arange(1, 11)\n",
    "data = ResultsInfo(DATA_PATH, model_types, prompt_types, level_range, save_path=\"results.csv\")\n",
    "data.dataframe['problem_name'] = data.dataframe['problem_name'].replace('BSP', 'SAS')\n",
    "\n",
    "\n",
    "# assume that for each groupby, it has the same number of results. Otherwise, it will be problematic\n",
    "# the raw weighted_acc does not apply the level weight\n",
    "summary_func = {\"level\": \"mean\", \"FR\": \"mean\", \"RA\": \"mean\", \"weighted_acc\": \"mean\"}\n",
    "baseSummary = SummarizeHelper(data.dataframe, summary_func)\n",
    "\n",
    "base_summary_info = baseSummary.summarize(pre_func=pre_func_for_agg_acc)\n",
    "base_summary_info[\"effective_rate\"] = 1 - base_summary_info[\"FR\"]\n",
    "base_summary_info.to_csv(\"base_summary_info.csv\", index=False)\n",
    "\n",
    "summary1 = SummarizeHelper(base_summary_info, summary_func)\n",
    "summary_info1 = summary1.summarize(\n",
    "    groupby_columns=[\"model_name\", \"problem_name\", \"problem_type\"],\n",
    "    summary_columns=[\"RA\", \"FR\", \"weighted_acc\", \"level\"],\n",
    "    pre_func=pre_func_common,\n",
    "    post_func=post_func_common\n",
    ").sort_values(by=\"model_name\", key=lambda x: x.map(model_order_mapper))\n",
    "summary_info1[\"effective_rate\"] = 1 - summary_info1[\"FR\"]\n",
    "summary_info1.to_csv(\"summary_info1.csv\", index=False)\n",
    "\n",
    "summary_info2 = summary1.summarize(\n",
    "    groupby_columns=[\"prompt_type\", \"model_name\"],\n",
    "    summary_columns=[\"RA\", \"FR\", \"weighted_acc\", \"level\"],\n",
    "    pre_func=pre_func_common,\n",
    "    post_func=post_func_common\n",
    ").sort_values(by=\"model_name\", key=lambda x: x.map(model_order_mapper))\n",
    "summary_info2[\"effective_rate\"] = 1 - summary_info2[\"FR\"]\n",
    "summary_info2.to_csv(\"summary_info2.csv\", index=False)\n",
    "\n",
    "summary_info3 = summary1.summarize(\n",
    "    groupby_columns=[\"model_name\", \"problem_type\", \"model_type\"],\n",
    "    summary_columns=[\"RA\", \"FR\", \"weighted_acc\", \"level\"],\n",
    "    pre_func=pre_func_common,\n",
    "    post_func=post_func_common\n",
    ").sort_values(by=\"model_name\", key=lambda x: x.map(model_order_mapper))\n",
    "summary_info3[\"effective_rate\"] = 1 - summary_info3[\"FR\"]\n",
    "summary_info3.to_csv(\"summary_info3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'figures' already exists.\n"
     ]
    }
   ],
   "source": [
    "# create folder\n",
    "folder_name = \"figures\"\n",
    "current_directory = os.getcwd()\n",
    "folder_path = os.path.join(current_directory, folder_name)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_name}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot spider graph \n",
    "summary_info_radar_model = summary1.summarize(\n",
    "    groupby_columns=[\"model_name\"],\n",
    "    summary_columns=[\"RA\", \"FR\", \"weighted_acc\", \"level\"],\n",
    "    pre_func=pre_func_common,\n",
    "    post_func=post_func_common\n",
    ").sort_values(by=\"model_name\", key=lambda x: x.map(model_order_mapper))\n",
    "summary_info_radar_model[\"effective_rate\"] = 1 - summary_info_radar_model[\"FR\"]\n",
    "summary_info_radar_model\n",
    "\n",
    "summary_info_radar_problem_type = summary1.summarize(\n",
    "    groupby_columns=[\"model_name\", \"problem_type\"],\n",
    "    summary_columns=[\"RA\", \"FR\", \"weighted_acc\", \"level\"],\n",
    "    pre_func=pre_func_common,\n",
    "    post_func=post_func_common\n",
    ").sort_values(by=\"model_name\", key=lambda x: x.map(model_order_mapper))\n",
    "summary_info_radar_problem_type[\"effective_rate\"] = 1 - summary_info_radar_problem_type[\"FR\"]\n",
    "summary_info_radar_problem_type = summary_info_radar_problem_type.pivot(index='model_name', columns='problem_type', values='weighted_acc')\n",
    "summary_info_radar_problem_type = summary_info_radar_problem_type.reset_index()\n",
    "summary_info_radar_problem_type = summary_info_radar_problem_type.sort_values(by = 'model_name', key=lambda x: x.map(model_order_mapper))\n",
    "summary_info_radar_problem_type\n",
    "\n",
    "# join the graph with overall following rate and RA with AA in different difficulty level\n",
    "summary_info_radar_join = pd.merge(summary_info_radar_model, summary_info_radar_problem_type, on=\"model_name\")\n",
    "summary_info_radar_join = summary_info_radar_join[[\"model_name\" , \"FR\" ,\"p\",\"np-cmp\", \"np-hard\",\"RA\"]]\n",
    "summary_info_radar_join[\"FR\"] = 1 - summary_info_radar_join[\"FR\"]\n",
    "summary_info_radar_join = summary_info_radar_join.rename(columns={'p': \"AA P\", \"np-cmp\" :'AA NP Complete', 'np-hard': 'AA NP Hard',\"FR\":\"ER\"})\n",
    "\n",
    "spider_helper = DrawHelper(summary_info_radar_join)\n",
    "spider_helper.plot_spider(\"ER\",\"AA P\", 'AA NP Complete', 'AA NP Hard',\"RA\")\n",
    "plt.savefig('figures/spider.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rq0 question hardness level recognition\n",
    "\n",
    "fig, _ = plt.subplots(3, 3, figsize=(15, 12))\n",
    "rq1_1_drawer = DrawHelper(base_summary_info)\n",
    "rq1_1_drawer.plot_heatmap('level', 'model_name', 'problem_name', 'RA', problem_order_mapper)\n",
    "fig.subplots_adjust(right=0.95)\n",
    "cbar_ax = fig.add_axes([0.96, 0.7, 0.02, 0.27])\n",
    "fig.colorbar(ax=fig.axes, cax=cbar_ax, mappable=fig.axes[0].collections[0], orientation='vertical')\n",
    "plt.savefig('figures/recognition_heatmap.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq0.1 recognition question level\n",
    "fig, _ = plt.subplots(figsize=(15, 12))\n",
    "rq1_drawer = DrawHelper(summary_info1)\n",
    "rq1_drawer.plot_heatmap_recognition()\n",
    "plt.savefig('figures/agg_recognition_heatmap.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq1\n",
    "rq1_drawer = DrawHelper(summary_info3)\n",
    "## rq1.1\n",
    "fig, axs = plt.subplots(figsize=(12, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1,1], height_ratios=[1],wspace=0.30)\n",
    "\n",
    "ax1 = plt.subplot(gs[0, 0])\n",
    "rq1_drawer.plot_line_chart('weighted_acc', 'problem_type', nphard_order_mapper,ax1)\n",
    "ax1.set_aspect(2)\n",
    "\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "rq1_drawer.plot_line_chart('effective_rate', 'problem_type', nphard_order_mapper,ax2)\n",
    "ax2.set_aspect(2)\n",
    "\n",
    "plt.savefig('figures/weighted_accuracy_effective_rate.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rq1.2\n",
    "fig, _ = plt.subplots(3, 3, figsize=(15, 12))\n",
    "rq1_1_drawer = DrawHelper(base_summary_info)\n",
    "temp = rq1_1_drawer.plot_heatmap('level', 'model_name', 'problem_name', 'weighted_acc', problem_order_mapper)\n",
    "fig.subplots_adjust(right=0.95)\n",
    "cbar_ax = fig.add_axes([0.96, 0.7, 0.02, 0.27])\n",
    "fig.colorbar(ax=fig.axes, cax=cbar_ax, mappable=fig.axes[0].collections[0], orientation='vertical')\n",
    "plt.savefig('figures/zeroshot_heatmap.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq2\n",
    "mapper = {\n",
    "    'figure+limited_text': 'Figure + Limited Text',\n",
    "    'full_text_only': 'Full Text Only',\n",
    "    'figure+full_text': 'Figure + Full Text'\n",
    "}\n",
    "\n",
    "summary_info2['prompt_type'] = summary_info2['prompt_type'].map(mapper)\n",
    "summary_info2.rename(columns={\"prompt_type\":\"Prompt Type\"})\n",
    "rq2_drawer = DrawHelper(summary_info2)\n",
    "fig = plt.figure()\n",
    "rq2_drawer.plot_bar_chart('model_name', 'weighted_acc', 'prompt_type')\n",
    "plt.savefig('figures/weighted_accuracy_prompt_type.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq3: finetune\n",
    "# it is not used yet\n",
    "# datapaths = [\"baseline\", \"finetune-1\", \"finetune-2\", \"finetune-3\", \"finetune-4\", \"finetune-5\"]\n",
    "# number_of_tunes = len(datapaths)\n",
    "# number_of_problems = 9\n",
    "# number_of_models = 2\n",
    "# model_problem_cl_values = np.zeros((number_of_tunes, number_of_models, number_of_problems))\n",
    "# problem_names = problem_order_mapper.keys()\n",
    "# for i, datapath in enumerate(datapaths):\n",
    "#     data = ResultsInfo(datapath, model_types, prompt_types, level_range)\n",
    "#     tmp_summary_info = baseSummary.summarize(\n",
    "#         summary_columns=[\"RA\", \"FR\", \"weighted_acc\", \"level\"],\n",
    "#         pre_func=pre_func_for_agg_acc,\n",
    "#     )\n",
    "#     tmp_summary = SummarizeHelper(tmp_summary_info, summary_func)\n",
    "#     tmp_summary_info1 = tmp_summary.summarize(\n",
    "#         groupby_columns=[\"model_name\", \"problem_name\"],\n",
    "#         summary_columns=[\"weighted_acc\", \"level\"],\n",
    "#         pre_func=pre_func_common,\n",
    "#         post_func=post_func_common\n",
    "#     ).sort_values(by=\"model_name\", key=lambda x: x.map(model_order_mapper))\n",
    "#     tmp_pivot = tmp_summary_info1.pivot(index=\"model_name\", columns=\"problem_name\", values=\"weighted_acc\")\n",
    "#     tmp_pivot = tmp_pivot[problem_names]\n",
    "#     model_problem_cl_values[i] = tmp_pivot.values\n",
    "\n",
    "# model_problem_cl = np.mean(np.diff(model_problem_cl_values, axis=0), axis=0)\n",
    "# model_problem_cl_df = pd.DataFrame(model_problem_cl, columns=problem_names, index=model_order_mapper.keys())\n",
    "# fig = plt.figure()\n",
    "# sns.heatmap(model_problem_cl_df, annot=True, cmap=\"Blues\", fmt=\".2f\")\n",
    "# plt.title(\"Continual Learnability (CL)\")\n",
    "# plt.ylabel(\"Model\")\n",
    "# plt.xlabel(\"Problem\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('figures/continual_learnability.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si650-course-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
